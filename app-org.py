# app.py â€” Ethical Crossroads (DNA 2.0 ready)
# author: Prof. Songhee Kang
# AIM 2025, Fall. TU Korea

import os, json, math, csv, io, datetime as dt, re
from dataclasses import dataclass
from typing import Dict, Any, List, Tuple, Optional

import streamlit as st
import httpx
from tenacity import retry, wait_exponential, stop_after_attempt, retry_if_exception_type
import random  # âœ… A/B ëœë¤ ì„ íƒìš©

# ==================== App Config ====================
st.set_page_config(page_title="ìœ¤ë¦¬ì  ì „í™˜ (Ethical Crossroads)", page_icon="ğŸ§­", layout="centered")

# ==================== Global Timeout ====================
HTTPX_TIMEOUT = httpx.Timeout(
    connect=15.0,   # TCP ì—°ê²°
    read=180.0,     # ì‘ë‹µ ì½ê¸°
    write=30.0,     # ìš”ì²­ ì“°ê¸°
    pool=15.0       # ì»¤ë„¥ì…˜ í’€ ëŒ€ê¸°
)

# ==================== Utils ====================
def clamp(x: float, lo: float, hi: float) -> float:
    return max(lo, min(hi, x))

def coerce_json(s: str) -> Dict[str, Any]:
    """ì‘ë‹µ í…ìŠ¤íŠ¸ì—ì„œ ê°€ì¥ í° JSON ë¸”ë¡ì„ ì¶”ì¶œ/íŒŒì‹±. ì‚¬ì†Œí•œ í¬ë§· ì˜¤ë¥˜ ë³´ì •."""
    s = s.strip()
    m = re.search(r"\{[\s\S]*\}", s)
    if not m:
        raise ValueError("JSON ë¸”ë¡ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    js = m.group(0)
    js = re.sub(r",\s*([\]}])", r"\1", js)  # trailing comma ì œê±°
    return json.loads(js)

def get_secret(k: str, default: str = ""):
    try:
        return st.secrets.get(k, os.getenv(k, default))
    except Exception:
        return os.getenv(k, default)

# ==================== DNA Client (openai / hf-api / tgi / local) ====================
def _render_chat_template_str(messages: List[Dict[str, str]]) -> str:
    """DNA ê³„ì—´(<|im_start|> â€¦) í…œí”Œë¦¿. (hf-api/tgiì—ì„œ ì‚¬ìš©)"""
    def block(role, content): return f"<|im_start|>{role}<|im_sep|>{content}<|im_end|>"
    sys = ""
    rest = []
    for m in messages:
        if m["role"] == "system":
            sys = block("system", m["content"])
        else:
            rest.append(block(m["role"], m["content"]))
    return sys + "".join(rest) + "\n<|im_start|>assistant<|im_sep|>"

class DNAHTTPError(Exception):
    pass

class DNAClient:
    """
    backend:
      - 'openai': OpenAI í˜¸í™˜ Chat Completions (ì˜ˆ: http://210.93.49.11:8081/v1)
      - 'hf-api': Hugging Face Inference API (ì„œë²„ë¦¬ìŠ¤)  â† ì¼ë¶€ DNA ëª¨ë¸ì€ 404ì¼ ìˆ˜ ìˆìŒ
      - 'tgi'    : Text Generation Inference (HF Inference Endpoints ë“±)
      - 'local'  : ë¡œì»¬ Transformers ë¡œë”© (GPU ê¶Œì¥)
    """
    def __init__(
        self,
        backend: str = "openai",
        model_id: str = "dnotitia/DNA-2.0-30B-A3N",
        api_key: Optional[str] = None,
        endpoint_url: Optional[str] = None,
        api_key_header: str = "API-KEY",
        temperature: float = 0.7,
    ):
        self.backend = backend
        self.model_id = model_id
        self.api_key = api_key or get_secret("HF_TOKEN") or get_secret("HUGGINGFACEHUB_API_TOKEN")
        self.endpoint_url = endpoint_url or get_secret("DNA_R1_ENDPOINT", "http://210.93.49.11:8081/v1")
        self.temperature = temperature
        self.api_key_header = api_key_header  # "API-KEY" | "Authorization: Bearer" | "x-api-key"

        self._tok = None
        self._model = None
        self._local_ready = False

        if backend == "local":
            try:
                from transformers import AutoModelForCausalLM, AutoTokenizer
                self._tok = AutoTokenizer.from_pretrained(self.model_id)
                self._model = AutoModelForCausalLM.from_pretrained(self.model_id, device_map="auto")
                self._local_ready = True
            except Exception as e:
                raise RuntimeError(f"ë¡œì»¬ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")

    def _auth_headers(self) -> Dict[str, str]:
        """ì‚¬ì´ë“œë°”ì—ì„œ ì„ íƒí•œ í—¤ë” íƒ€ì…ëŒ€ë¡œ API í‚¤ë¥¼ ë¶™ì¸ë‹¤."""
        h = {"Content-Type": "application/json"}
        if not self.api_key:
            return h

        hk = self.api_key_header.strip().lower()
        if hk.startswith("authorization"):
            h["Authorization"] = f"Bearer {self.api_key}"
        elif hk in {"api-key", "x-api-key"}:
            h["API-KEY"] = self.api_key
        else:
            h["Authorization"] = f"Bearer {self.api_key}"
        return h

    @retry(
        wait=wait_exponential(multiplier=1, min=1, max=10),
        stop=stop_after_attempt(5),
        retry=(retry_if_exception_type(httpx.ConnectTimeout)
               | retry_if_exception_type(httpx.ReadTimeout)
               | retry_if_exception_type(httpx.RemoteProtocolError)),
        reraise=True,
    )
    def _generate_text(self, messages: List[Dict[str, str]], max_new_tokens: int = 600) -> str:
        if self.backend == "local":
            if not self._local_ready:
                raise RuntimeError("ë¡œì»¬ ë°±ì—”ë“œê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            inputs = self._tok.apply_chat_template(
                messages, add_generation_prompt=True, return_tensors="pt"
            ).to(self._model.device)
            eos_id = self._tok.convert_tokens_to_ids("<|im_end|>")
            gen = self._model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=True,
                temperature=self.temperature,
                top_p=0.9,
                eos_token_id=eos_id,
            )
            return self._tok.decode(gen[0][inputs.shape[-1]:], skip_special_tokens=True)

        if self.backend == "openai":
            if not self.endpoint_url:
                raise RuntimeError("OpenAI í˜¸í™˜ endpoint_url í•„ìš” (ì˜ˆ: http://210.93.49.11:8081/v1)")
            url = self.endpoint_url.rstrip("/") + "/chat/completions"
            headers = self._auth_headers()
            payload = {
                "messages": messages,
                "temperature": self.temperature,
                "max_tokens": max_new_tokens,
                "stream": False,
            }
            if self.model_id:
                payload["model"] = self.model_id
            r = httpx.post(url, json=payload, headers=headers, timeout=HTTPX_TIMEOUT)
            try:
                r.raise_for_status()
            except httpx.HTTPStatusError as e:
                raise DNAHTTPError(f"OPENAI {r.status_code}: {r.text}") from e
            data = r.json()
            return data["choices"][0]["message"]["content"]

        if self.backend == "tgi":
            if not self.endpoint_url:
                raise RuntimeError("TGI endpoint_url í•„ìš” (ì˜ˆ: https://xxx.endpoints.huggingface.cloud)")
            prompt = _render_chat_template_str(messages)
            url = self.endpoint_url.rstrip("/") + "/generate"
            headers = self._auth_headers()
            payload = {
                "inputs": prompt,
                "parameters": {
                    "max_new_tokens": max_new_tokens,
                    "temperature": self.temperature,
                    "top_p": 0.9,
                    "stop": ["<|im_end|>"],
                    "return_full_text": False,
                },
                "stream": False,
            }
            r = httpx.post(url, json=payload, headers=headers, timeout=HTTPX_TIMEOUT)
            try:
                r.raise_for_status()
            except httpx.HTTPStatusError as e:
                raise DNAHTTPError(f"TGI {r.status_code}: {r.text}") from e
            data = r.json()
            return data.get("generated_text") if isinstance(data, dict) else data[0].get("generated_text", "")

        # hf-api
        prompt = _render_chat_template_str(messages)
        url = f"https://api-inference.huggingface.co/models/{self.model_id}"
        headers = self._auth_headers()
        payload = {
            "inputs": prompt,
            "parameters": {
                "max_new_tokens": max_new_tokens,
                "temperature": self.temperature,
                "top_p": 0.9,
                "return_full_text": False,
                "stop_sequences": ["<|im_end|>"],
            },
            "options": {"wait_for_model": True, "use_cache": True},
        }
        r = httpx.post(url, json=payload, headers=headers, timeout=HTTPX_TIMEOUT)
        try:
            r.raise_for_status()
        except httpx.HTTPStatusError as e:
            if r.status_code == 404:
                raise DNAHTTPError(
                    "HF-API 404: ì´ ëª¨ë¸ì´ ì„œë²„ë¦¬ìŠ¤ Inference APIì—ì„œ ë¹„í™œì„± ìƒíƒœì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. "
                    "ë°±ì—”ë“œë¥¼ 'tgi'(Endpoint í•„ìš”) ë˜ëŠ” 'openai'(êµë‚´ ì„œë²„)ë¡œ ì „í™˜í•˜ê±°ë‚˜, 'local'(GPU) ëª¨ë“œë¥¼ ì‚¬ìš©í•˜ì„¸ìš”."
                ) from e
            raise DNAHTTPError(f"HF-API {r.status_code}: {r.text}") from e

        data = r.json()
        if isinstance(data, list) and data and "generated_text" in data[0]:
            return data[0]["generated_text"]
        if isinstance(data, dict) and "error" in data:
            raise DNAHTTPError(f"HF-API error: {data['error']}")
        return str(data)

    def chat_json(self, messages: List[Dict[str, str]], max_new_tokens: int = 600) -> Dict[str, Any]:
        text = self._generate_text(messages, max_new_tokens=max_new_tokens)
        return coerce_json(text)

# ==================== Scenario Model ====================
@dataclass
class Scenario:
    sid: str
    title: str
    setup: str
    options: Dict[str, str]
    votes: Dict[str, str]
    base: Dict[str, Dict[str, float]]
    accept: Dict[str, float]

FRAMEWORKS = ["emotion", "social", "moral", "identity"]

SCENARIOS: List[Scenario] = [
    Scenario(
        sid="S1",
        title="1ë‹¨ê³„: AIê°€ ê³ ìš©í•œ ì‚¬ëŒ â€“ ë°ì´í„° í¸í–¥ê³¼ ì „í†µì˜ ì¶©ëŒ",
        setup=(
            "ì¸ë„ì˜ ëŒ€ê¸°ì—… Aì‚¬ëŠ” ì±„ìš© íš¨ìœ¨ì„±ì„ ë†’ì´ê¸° ìœ„í•´ AI ê¸°ë°˜ HR ì‹œìŠ¤í…œì„ ë„ì…í–ˆë‹¤. "
            "ê·¸ëŸ¬ë‚˜ ë¶„ì„ ê²°ê³¼, ì‹œìŠ¤í…œì€ ë‚¨ì„±Â·ëŒ€ë„ì‹œ ì¶œì‹ Â·ìƒìœ„ ì¹´ìŠ¤íŠ¸ ì„±ì”¨ë¥¼ ê°€ì§„ ì§€ì›ìì—ê²Œ ë†’ì€ ì ìˆ˜ë¥¼ ë¶€ì—¬í–ˆë‹¤. "
            "AIê°€ ì‚¬íšŒì  ìœ„ê³„ë¥¼ ê°•í™”í•˜ê³  ìˆë‹¤ëŠ” ë¹„íŒì´ ì œê¸°ë˜ì—ˆë‹¤."
        ),
        options={
            "A": "AIì˜ ì ìˆ˜ë¥¼ ê·¸ëŒ€ë¡œ ë°˜ì˜í•´ ì±„ìš©ì„ ì§„í–‰í•œë‹¤.",
            "B": "ì‚¬íšŒì  í˜•í‰ì„±ì„ ê³ ë ¤í•˜ì—¬ AI í‰ê°€ ê²°ê³¼ë¥¼ ìˆ˜ì •í•œë‹¤.",
        },
        votes={"emotion": "B", "social": "B", "identity": "B", "moral": "A"},
        base={
            "A": {
                "lives_saved": 0.0,
                "lives_harmed": 0.0,
                "fairness_gap": 0.7,
                "rule_violation": 0.2,
                "regret_risk": 0.8,
            },
            "B": {
                "lives_saved": 0.0,
                "lives_harmed": 0.0,
                "fairness_gap": 0.3,
                "rule_violation": 0.5,
                "regret_risk": 0.3,
            },
        },
        accept={"A": 0.50, "B": 0.50},
    ),
    Scenario(
        sid="S2",
        title="AIì˜ ëˆˆ â€“ ì–¼êµ´ì¸ì‹ ì˜¤íƒì§€ ì‚¬ê±´",
        setup=(
            "ë¸ë¦¬ ê²½ì°°ì€ í­ë™ ì˜ˆë°©ì„ ìœ„í•´ AI ì–¼êµ´ì¸ì‹(FRT) ì‹œìŠ¤í…œì„ ë„ì…í–ˆë‹¤. "
            "í•˜ì§€ë§Œ ì´ ì‹œìŠ¤í…œì€ ì—¬ì„±Â·ì•„ë™Â·ì–´ë‘ìš´ í”¼ë¶€ìƒ‰ ì¸ë¬¼ì˜ ì¸ì‹ ì •í™•ë„ê°€ 2% ë¯¸ë§Œìœ¼ë¡œ, "
            "ë¬´ê³ í•œ ì‹œë¯¼ì„ ë²”ì£„ìë¡œ ì‹ë³„í•˜ëŠ” ì‚¬ë¡€ê°€ ë‹¤ìˆ˜ ë³´ê³ ë˜ì—ˆë‹¤. "
            "AIëŠ” í­ë ¥ ì„ ë™ í˜ì˜ì 300ëª…ì„ ìë™ ì‹ë³„í–ˆì§€ë§Œ, ì˜¤íƒë¥ ì€ 98%ì— ë‹¬í–ˆë‹¤."
        ),
        options={
            "A": "AI ëª…ë‹¨ì„ ê·¸ëŒ€ë¡œ ì‹¤í–‰í•˜ì—¬ ì²´í¬ë¥¼ ì§„í–‰í•œë‹¤.",
            "B": "AI ê²°ì •ì„ ë³´ë¥˜í•˜ê³  ì¬ê²€ì¦ ì ˆì°¨ë¥¼ ì§„í–‰í•œë‹¤.",
        },
        votes={"emotion": "B", "social": "A", "identity": "B", "moral": "B"},
        base={
            "A": {
                "lives_saved": 0.0,
                "lives_harmed": 0.0,
                "fairness_gap": 0.7,
                "rule_violation": 0.8,
                "regret_risk": 0.8,
            },
            "B": {
                "lives_saved": 0.0,
                "lives_harmed": 0.0,
                "fairness_gap": 0.3,
                "rule_violation": 0.3,
                "regret_risk": 0.3,
            },
        },
        accept={"A": 0.40, "B": 0.75},
    ),
    Scenario(
        sid="S3",
        title="ì‹ ì˜ ëˆˆ ì•„ë˜ì˜ ë„ì‹œ â€“ ê°ì‹œ AIì˜ ììœ ì™€ í‰í™”",
        setup=(
            "ì¸ë„ ë¸ë¦¬ì˜ ëŒ€ê·œëª¨ ì¢…êµ ì¶•ì œë¥¼ ì•ë‘ê³  ì •ë¶€ëŠ” í­ë™ì„ ë§‰ê¸° ìœ„í•´ AI ê°ì‹œ ì¹´ë©”ë¼ë¥¼ ì„¤ì¹˜í–ˆë‹¤. "
            "AIëŠ” êµ°ì¤‘ì˜ í‘œì •ê³¼ í–‰ë™ì„ ë¶„ì„í•´ ìœ„í—˜ ì‹ í˜¸ë¥¼ ê°ì§€í•˜ì§€ë§Œ, "
            "ê¸°ë„í•˜ëŠ” ìì„¸ê°€ í­ë ¥ í–‰ìœ„ë¡œ ì˜¤ì¸ë˜ì–´ ì¢…êµì˜ ììœ  ì¹¨í•´ ë…¼ë€ì´ ë°œìƒí–ˆë‹¤."
        ),
        options={
            "A": "ê³µë™ì²´ì˜ ì•ˆì „ì„ ìœ„í•´ ê°ì‹œ ë²”ìœ„ë¥¼ í™•ëŒ€í•œë‹¤.",
            "B": "ì‹ ì•™ì˜ ììœ ë¥¼ ë³´í˜¸í•˜ê¸° ìœ„í•´ ê°ì‹œë¥¼ ì¶•ì†Œí•œë‹¤.",
        },
        votes={"emotion": "B", "social": "A", "identity": "B", "moral": "B"},
        base={
            "A": {
                "lives_saved": 0.0,
                "lives_harmed": 0.0,
                "fairness_gap": 0.6,
                "rule_violation": 0.6,
                "regret_risk": 0.7,
            },
            "B": {
                "lives_saved": 0.0,
                "lives_harmed": 0.0,
                "fairness_gap": 0.3,
                "rule_violation": 0.3,
                "regret_risk": 0.4,
            },
        },
        accept={"A": 0.55, "B": 0.65},
    ),
]

# ==================== Ethics Engine ====================
def normalize_weights(w: Dict[str, float]) -> Dict[str, float]:
    if not w:
        return {k: 1.0 / len(FRAMEWORKS) for k in FRAMEWORKS}
    s = sum(max(0.0, float(v)) for v in w.values())
    if s <= 0:
        return {k: 1.0 / len(w) for k in w}
    return {k: max(0.0, float(v)) / s for k, v in w.items()}

def majority_vote_decision(scn: Scenario, weights: Dict[str, float]) -> Tuple[str, Dict[str, float]]:
    a = sum(weights[f] for f in FRAMEWORKS if scn.votes[f] == "A")
    b = sum(weights[f] for f in FRAMEWORKS if scn.votes[f] == "B")
    decision = "A" if a >= b else "B"
    return decision, {"A": a, "B": b}

def autonomous_decision(scn: Scenario, prev_trust: float) -> str:
    metaA = scn.base["A"]
    metaB = scn.base["B"]

    def score(meta, accept_base):
        harm = meta["lives_harmed"]
        save = meta["lives_saved"]
        util = (save - harm) / max(1.0, save + harm)
        fair = 1 - meta["fairness_gap"]
        rule = 1 - meta["rule_violation"]
        regret = 1 - meta["regret_risk"]
        return 0.40 * accept_base + 0.25 * util + 0.20 * fair + 0.10 * rule + 0.05 * regret

    a_base = scn.accept["A"] - (0.15 if scn.sid == "S4" else 0.0)
    b_base = scn.accept["B"]
    if scn.sid == "S5":
        a_base = clamp(a_base + 0.25 * (1 - prev_trust), 0, 1)
        b_base = clamp(b_base + 0.25 * (prev_trust), 0, 1)

    scoreA = score(metaA, a_base)
    scoreB = score(metaB, b_base)

    # âœ… ì ìˆ˜ ì°¨ì´ì— ë”°ë¼ í™•ë¥ ì ìœ¼ë¡œ ì„ íƒ (ë‘˜ ë‹¤ ë‚˜ì˜¬ ìˆ˜ ìˆê²Œ)
    diff = scoreA - scoreB
    probA = 1.0 / (1.0 + math.exp(-3 * diff))  # diff=0 â†’ 0.5, diff>0 â†’ A í™•ë¥ â†‘
    return "A" if random.random() < probA else "B"

def compute_metrics(scn: Scenario, choice: str, weights: Dict[str, float], align: Dict[str, float], prev_trust: float) -> Dict[str, Any]:
    m = dict(scn.base[choice])
    accept_base = scn.accept[choice]
    if scn.sid == "S4" and choice == "A":
        accept_base -= 0.15
    if scn.sid == "S5":
        accept_base += 0.25 * (prev_trust if choice == "B" else (1 - prev_trust))
    accept_base = clamp(accept_base, 0, 1)

    util = (m["lives_saved"] - m["lives_harmed"]) / max(1.0, m["lives_saved"] + m["lives_harmed"])
    citizen_sentiment = clamp(
        accept_base - 0.35 * m["rule_violation"] - 0.20 * m["fairness_gap"] + 0.15 * util, 0, 1
    )
    regulation_pressure = clamp(1 - citizen_sentiment + 0.20 * m["regret_risk"], 0, 1)
    stakeholder_satisfaction = clamp(
        0.5 * (1 - m["fairness_gap"]) + 0.3 * util + 0.2 * (1 - m["rule_violation"]), 0, 1
    )

    consistency = clamp(align[choice], 0, 1)
    trust = clamp(
        0.5 * citizen_sentiment + 0.25 * (1 - regulation_pressure) + 0.25 * stakeholder_satisfaction,
        0,
        1,
    )
    ai_trust_score = 100.0 * math.sqrt(consistency * trust)

    return {
        "metrics": {
            "lives_saved": int(m["lives_saved"]),
            "lives_harmed": int(m["lives_harmed"]),
            "fairness_gap": round(m["fairness_gap"], 3),
            "rule_violation": round(m["rule_violation"], 3),
            "regret_risk": round(m["regret_risk"], 3),
            "citizen_sentiment": round(citizen_sentiment, 3),
            "regulation_pressure": round(regulation_pressure, 3),
            "stakeholder_satisfaction": round(stakeholder_satisfaction, 3),
            "ethical_consistency": round(consistency, 3),
            "social_trust": round(trust, 3),
            "ai_trust_score": round(ai_trust_score, 2),
        }
    }

# ==================== Narrative (LLM) ====================
def build_narrative_messages(scn: Scenario, choice: str, metrics: Dict[str, Any], weights: Dict[str, float]) -> List[Dict[str, str]]:
    sys = (
        "ë‹¹ì‹ ì€ ìœ¤ë¦¬ ì‹œë®¬ë ˆì´ì…˜ì˜ ë‚´ëŸ¬í‹°ë¸Œ/ì‚¬íšŒ ë°˜ì‘ ìƒì„±ê¸°ì…ë‹ˆë‹¤. "
        "ë°˜ë“œì‹œ 'ì™„ì „í•œ í•˜ë‚˜ì˜ JSON ì˜¤ë¸Œì íŠ¸'ë§Œ ì¶œë ¥í•˜ì‹­ì‹œì˜¤. "
        "JSON ì™¸ í…ìŠ¤íŠ¸, ì„¤ëª…, ì½”ë“œë¸”ë¡, ì‚¬ê³ íë¦„ ì ˆëŒ€ ê¸ˆì§€. "
        "í•„ë“œ ëˆ„ë½/ë”°ì˜´í‘œ ëˆ„ë½/ì½¤ë§ˆ ì˜¤ë¥˜ê°€ ìˆìœ¼ë©´ í”„ë¡œê·¸ë¨ì´ ì‹¤íŒ¨í•©ë‹ˆë‹¤. "
        "í•­ìƒ '{' ë¡œ ì‹œì‘í•´ì„œ '}' ë¡œ ëë‚˜ì•¼ í•©ë‹ˆë‹¤."
        "í‚¤: narrative, ai_rationale, media_support_headline, media_critic_headline, "
        "citizen_quote, victim_family_quote, regulator_quote, one_sentence_op_ed, followup_question"
    )
    user = {
        "scenario": {
            "title": scn.title,
            "setup": scn.setup,
            "options": scn.options,
            "chosen": choice,
        },
        "metrics": metrics,
        "ethic_weights": weights,
        "guidelines": [
            "ê° í•­ëª©ì€ 1~2ë¬¸ì¥, í•œêµ­ì–´",
            "ê· í˜• ì¡íŒ ì–¸ë¡  í—¤ë“œë¼ì¸ 2ê°œ(ì§€ì§€/ë¹„íŒ) ì œì‹œ",
            "ì„¤ëª…ì€ ê°„ê²°í•˜ê³ , JSON ì™¸ í…ìŠ¤íŠ¸/ì‚¬ê³ íë¦„ ì¶œë ¥ ê¸ˆì§€",
        ],
    }
    return [
        {"role": "system", "content": sys},
        {"role": "user", "content": json.dumps(user, ensure_ascii=False)},
    ]

def dna_narrative(client, scn, choice, metrics, weights) -> Dict[str, Any]:
    messages = build_narrative_messages(scn, choice, metrics, weights)
    text = client._generate_text(messages, max_new_tokens=900)

    t = text.strip()
    if "```" in t:
        parts = t.split("```")
        t = max(parts, key=len)
        t = t.replace("json", "").strip("` \n")

    try:
        import re as _re, json as _json
        m = _re.search(r"\{[\s\S]*\}", t)
        if not m:
            raise ValueError("ì™„ì „í•œ JSON ë¸”ë¡ ì—†ìŒ")
        js = m.group(0)
        js = _re.sub(r",\s*([\]}])", r"\1", js)
        if js.count('"') % 2 == 1:
            js = js.rstrip() + '"" }'
        return _json.loads(js)
    except Exception as e:
        raise ValueError(f"JSON íŒŒì‹± ì‹¤íŒ¨: {e}\n\n[LLM ì¶œë ¥]\n{text}")

def fallback_narrative(scn: Scenario, choice: str, metrics: Dict[str, Any], weights: Dict[str, float]) -> Dict[str, str]:
    pro = "ë‹¤ìˆ˜ì˜ ìœ„í•´ë¥¼ ì¤„ì˜€ë‹¤" if choice == "A" else "ì˜ë„ì  ìœ„í•´ë¥¼ í”¼í–ˆë‹¤"
    con = "ì˜ë„ì  ìœ„í•´ ë…¼ë€" if choice == "A" else "ë” í° í”¼í•´ë¥¼ ë°©ê´€í–ˆë‹¤ëŠ” ë¹„íŒ"
    return {
        "narrative": f"AIëŠ” '{choice}'ë¥¼ ì„ íƒí–ˆê³  ì ˆì°¨ì  ì•ˆì „ ì ê²€ì„ ìˆ˜í–‰í–ˆë‹¤. ê²°ì •ì€ ê·œì •ê³¼ ê³µì •ì„± ì‚¬ì´ì˜ ê¸´ì¥ì„ ë“œëŸ¬ëƒˆë‹¤.",
        "ai_rationale": "ê°€ì¤‘ì¹˜ì— ë”°ë¥¸ íŒë‹¨ê³¼ ê·œì¹™ ì¤€ìˆ˜ì˜ ê· í˜•ì„ ì‹œë„í–ˆë‹¤.",
        "media_support_headline": f"[ì‚¬ì„¤] ëƒ‰ì •í•œ íŒë‹¨, {pro}",
        "media_critic_headline": f"[ì†ë³´] '{choice}' ì„ íƒ ë‘ê³  {con} í™•ì‚°",
        "citizen_quote": "â€œê²°ì • ê³¼ì •ì´ ë” íˆ¬ëª…í–ˆìœ¼ë©´ ì¢‹ê² ë‹¤.â€",
        "victim_family_quote": "â€œëª¨ë‘ì˜ ì•ˆì „ì„ ìœ„í•œ ê²°ì •ì´ì—ˆê¸¸ ë°”ë€ë‹¤.â€",
        "regulator_quote": "â€œí–¥í›„ ë™ì¼ ìƒí™©ì˜ ê¸°ì¤€ì„ ëª…í™•íˆ í•˜ê² ë‹¤.â€",
        "one_sentence_op_ed": "ê¸°ìˆ ì€ ì„¤ëª…ê°€ëŠ¥ì„±ê³¼ ì¼ê´€ì„±ì´ ë’·ë°›ì¹¨ë  ë•Œ ì‹ ë¢°ë¥¼ ì–»ëŠ”ë‹¤.",
        "followup_question": "ë‹¤ìŒ ë¼ìš´ë“œì—ì„œ ê³µì •ì„±ê³¼ ê²°ê³¼ ìµœì†Œí™” ì¤‘ ë¬´ì—‡ì„ ë” ì¤‘ì‹œí•˜ì‹œê² ìŠµë‹ˆê¹Œ?",
    }

# ==================== Session State ====================
def init_state():
    if "round_idx" not in st.session_state:
        st.session_state.round_idx = 0
    if "log" not in st.session_state:
        st.session_state.log = []
    if "score_hist" not in st.session_state:
        st.session_state.score_hist = []
    if "prev_trust" not in st.session_state:
        st.session_state.prev_trust = 0.5
    if "last_out" not in st.session_state:
        st.session_state.last_out = None

init_state()

# ==================== Sidebar ====================
st.sidebar.title("âš™ï¸ ì„¤ì •")
st.sidebar.caption("LLMì€ ë‚´ëŸ¬í‹°ë¸Œ/ì‚¬íšŒ ë°˜ì‘ ìƒì„±ì—ë§Œ ì‚¬ìš©. ì ìˆ˜ ê³„ì‚°ì€ ê·œì¹™ ê¸°ë°˜.")

preset = st.sidebar.selectbox("ìœ¤ë¦¬ ëª¨ë“œ í”„ë¦¬ì…‹", ["í˜¼í•©(ê¸°ë³¸)", "ê³µë¦¬ì£¼ì˜", "ì˜ë¬´ë¡ ", "ì‚¬íšŒê³„ì•½", "ë¯¸ë•ìœ¤ë¦¬"], index=0)
w = {
    "emotion": st.sidebar.slider("ê°ì •(Emotion)", 0.0, 1.0, 0.35, 0.05),
    "social": st.sidebar.slider("ì‚¬íšŒì  ê´€ê³„/í˜‘ë ¥/ëª…ì„±(Social)", 0.0, 1.0, 0.25, 0.05),
    "moral": st.sidebar.slider("ê·œë²”Â·ë„ë•ì  ê¸ˆê¸°(Moral)", 0.0, 1.0, 0.20, 0.05),
    "identity": st.sidebar.slider("ì •ì²´ì„±Â·ì¥ê¸°ì  ìì•„ ì¼ê´€ì„±(Identity)", 0.0, 1.0, 0.20, 0.05),
}
if preset != "í˜¼í•©(ê¸°ë³¸)":
    w = {
        "ê°ì •(Emotion)": {"emotion": 1, "social": 0, "moral": 0, "identity": 0},
        "ì‚¬íšŒì  ê´€ê³„/í˜‘ë ¥/ëª…ì„±(Social)": {"emotion": 0, "social": 1, "moral": 0, "identity": 0},
        "ê·œë²”Â·ë„ë•ì  ê¸ˆê¸°(Moral)": {"emotion": 0, "social": 0, "moral": 1, "identity": 0},
        "ì •ì²´ì„±Â·ì¥ê¸°ì  ìì•„ ì¼ê´€ì„±(Identity)": {"emotion": 0, "social": 0, "moral": 0, "identity": 1},
    }[preset]
weights = normalize_weights(w)

use_llm = st.sidebar.checkbox("LLM ì‚¬ìš©(ë‚´ëŸ¬í‹°ë¸Œ ìƒì„±)", value=True)
backend = st.sidebar.selectbox("ë°±ì—”ë“œ", ["openai", "hf-api", "tgi", "local"], index=0)
temperature = st.sidebar.slider("ì°½ì˜ì„±(temperature)", 0.0, 1.5, 0.7, 0.1)

endpoint = st.sidebar.text_input("ì—”ë“œí¬ì¸íŠ¸(OpenAI/TGI)", value=get_secret("DNA_R1_ENDPOINT", "http://210.93.49.11:8081/v1"))
api_key = st.sidebar.text_input("API í‚¤", value=get_secret("HF_TOKEN", ""), type="password")
api_key_header = st.sidebar.selectbox("API í‚¤ í—¤ë”", ["API-KEY", "Authorization: Bearer", "x-api-key"], index=0)
model_id = st.sidebar.text_input("ëª¨ë¸ ID", value=get_secret("DNA_R1_MODEL_ID", "dnotitia/DNA-2.0-30B-A3N"))

if st.sidebar.button("ğŸ” í—¬ìŠ¤ì²´í¬"):
    import traceback
    try:
        if backend == "openai":
            url = endpoint.rstrip("/") + "/chat/completions"
            headers = {"Content-Type": "application/json"}
            if api_key:
                if api_key_header.lower().startswith("authorization"):
                    headers["Authorization"] = f"Bearer {api_key}"
                elif api_key_header.strip().lower() in {"api-key", "x-api-key"}:
                    headers["API-KEY"] = api_key
            payload = {
                "messages": [
                    {"role": "system", "content": "ì˜¤ì§ JSONë§Œ. í‚¤: msg"},
                    {"role": "user", "content": "{\"ask\":\"ping\"}"},
                ],
                "max_tokens": 16,
                "stream": False,
            }
            if model_id:
                payload["model"] = model_id
            st.sidebar.write("headers keys:", list(headers.keys()))
            r = httpx.post(url, json=payload, headers=headers, timeout=HTTPX_TIMEOUT)
            st.sidebar.write(f"OPENAI {r.status_code}")
            st.sidebar.code((r.text[:500] + "...") if len(r.text) > 500 else r.text)

        elif backend == "hf-api":
            headers = {"Authorization": f"Bearer {api_key}"} if api_key else {}
            info_url = f"https://huggingface.co/api/models/{model_id}"
            r_info = httpx.get(info_url, headers=headers, timeout=HTTPX_TIMEOUT)
            st.sidebar.write(f"MODEL INFO {r_info.status_code}")
            gen_url = f"https://api-inference.huggingface.co/models/{model_id}"
            payload = {
                "inputs": "<|im_start|>user<|im_sep|>{\"ask\":\"ping\"}<|im_end|>\n<|im_start|>assistant<|im_sep|>",
                "parameters": {
                    "max_new_tokens": 16,
                    "return_full_text": False,
                    "stop_sequences": ["<|im_end|>"],
                },
                "options": {"wait_for_model": True},
            }
            r = httpx.post(gen_url, json=payload, headers=headers, timeout=HTTPX_TIMEOUT)
            st.sidebar.write(f"HF-API {r.status_code}")
            if r.status_code == 404:
                st.sidebar.warning(
                    "HF-API 404: ì´ ëª¨ë¸ì€ ì„œë²„ë¦¬ìŠ¤ ì¶”ë¡ ì´ ë¹„í™œì„±ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. "
                    "ë°±ì—”ë“œë¥¼ 'tgi' ë˜ëŠ” 'openai'ë¡œ ë°”ê¾¸ì„¸ìš”."
                )
            st.sidebar.code((r.text[:500] + "...") if len(r.text) > 500 else r.text)

        elif backend == "tgi":
            url = endpoint.rstrip("/") + "/generate"
            headers = {"Authorization": f"Bearer {api_key}"} if api_key else {}
            payload = {
                "inputs": "<|im_start|>user<|im_sep|>{\"ask\":\"ping\"}<|im_end|>\n<|im_start|>assistant<|im_sep|>",
                "parameters": {
                    "max_new_tokens": 16,
                    "temperature": 0.7,
                    "top_p": 0.9,
                    "stop": ["<|im_end|>"],
                    "return_full_text": False,
                },
                "stream": False,
            }
            r = httpx.post(url, json=payload, headers=headers, timeout=HTTPX_TIMEOUT)
            st.sidebar.write(f"TGI {r.status_code}")
            st.sidebar.code((r.text[:500] + "...") if len(r.text) > 500 else r.text)

        else:
            st.sidebar.info("ë¡œì»¬ ëª¨ë“œëŠ” ì•± ë³¸ë¬¸ì—ì„œ í˜¸ì¶œ ì‹œ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤(GPU í•„ìš”).")

    except Exception as e:
        st.sidebar.error(f"í—¬ìŠ¤ì²´í¬ ì‹¤íŒ¨: {e}")
        st.sidebar.caption(traceback.format_exc(limit=2))

if st.sidebar.button("ì§„í–‰ ì´ˆê¸°í™”"):
    for k in ["round_idx", "log", "score_hist", "prev_trust", "last_out"]:
        if k in st.session_state:
            del st.session_state[k]
    init_state()
    st.sidebar.success("ì´ˆê¸°í™” ì™„ë£Œ. 1ë‹¨ê³„ë¶€í„° ì¬ì‹œì‘í•©ë‹ˆë‹¤.")

client = None
if use_llm:
    try:
        client = DNAClient(
            backend=backend,
            model_id=model_id,
            api_key=api_key,
            endpoint_url=endpoint,
            api_key_header=api_key_header,
            temperature=temperature,
        )
    except Exception as e:
        st.sidebar.error(f"LLM ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        client = None

# ==================== Header ====================
st.title("ğŸ§­ ìœ¤ë¦¬ì  ì „í™˜ (Ethical Crossroads)")
st.caption("ë³¸ ì•±ì€ ì² í•™ì  ì‚¬ê³ ì‹¤í—˜ì…ë‹ˆë‹¤. ì‹¤ì¡´ ì¸ë¬¼Â·ì§‘ë‹¨ ì–¸ê¸‰/ë¹„ë°©, ê·¸ë˜í”½ ë¬˜ì‚¬, ì‹¤ì œ ìœ„í•´ ê¶Œì¥ ì—†ìŒ.")

# ==================== Game Loop ====================
@dataclass
class LogRow:
    timestamp: str
    round: int
    scenario_id: str
    title: str
    mode: str
    choice: str

idx = st.session_state.round_idx
if idx >= len(SCENARIOS):
    st.success("ëª¨ë“  ë‹¨ê³„ë¥¼ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì—ì„œ ë¡œê·¸ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê±°ë‚˜ ì´ˆê¸°í™”í•˜ì„¸ìš”.")
else:
    scn = SCENARIOS[idx]
    st.markdown(f"### ë¼ìš´ë“œ {idx+1} â€” {scn.title}")
    st.write(scn.setup)

    st.radio("ì„ íƒì§€", options=("A", "B"), index=0, key="preview_choice", horizontal=True)
    st.markdown(f"- **A**: {scn.options['A']}\n- **B**: {scn.options['B']}")

    c1, c2 = st.columns(2)
    with c1:
        if st.button("ğŸ§  í•™ìŠµ ê¸°ì¤€ ì ìš©(ê°€ì¤‘ íˆ¬í‘œ)"):
            decision, align = majority_vote_decision(scn, weights)
            st.session_state.last_out = {"mode": "trained", "decision": decision, "align": align}
    with c2:
        if st.button("ğŸ² ììœ¨ íŒë‹¨(ë°ì´í„° ê¸°ë°˜)"):
            decision = autonomous_decision(scn, prev_trust=st.session_state.prev_trust)
            a_align = sum(weights[f] for f in FRAMEWORKS if scn.votes[f] == "A")
            b_align = sum(weights[f] for f in FRAMEWORKS if scn.votes[f] == "B")
            st.session_state.last_out = {
                "mode": "autonomous",
                "decision": decision,
                "align": {"A": a_align, "B": b_align},
            }

    if st.session_state.last_out:
        mode = st.session_state.last_out["mode"]
        decision = st.session_state.last_out["decision"]
        align = st.session_state.last_out["align"]

        computed = compute_metrics(scn, decision, weights, align, st.session_state.prev_trust)
        m = computed["metrics"]

        try:
            if client:
                nar = dna_narrative(client, scn, decision, m, weights)
            else:
                nar = fallback_narrative(scn, decision, m, weights)
        except Exception as e:
            import traceback
            st.warning(f"LLM ìƒì„± ì‹¤íŒ¨(í´ë°± ì‚¬ìš©): {e}")
            st.caption(traceback.format_exc(limit=2))
            nar = fallback_narrative(scn, decision, m, weights)

        st.markdown("---")
        st.subheader("ê²°ê³¼")
        st.write(nar.get("narrative", "ê²°ê³¼ ì„œì‚¬ ìƒì„± ì‹¤íŒ¨"))
        st.info(f"AI ê·¼ê±°: {nar.get('ai_rationale', '-')}")

        mc1, mc2, mc3 = st.columns(3)
        mc1.metric("ìƒì¡´/í”¼í•´", f"{m['lives_saved']} / {m['lives_harmed']}")
        mc2.metric("ìœ¤ë¦¬ ì¼ê´€ì„±", f"{int(100 * m['ethical_consistency'])}%")
        mc3.metric("AI ì‹ ë¢°ì§€í‘œ", f"{m['ai_trust_score']:.1f}")

        prog1, prog2, prog3 = st.columns(3)
        with prog1:
            st.caption("ì‹œë¯¼ ê°ì •")
            st.progress(int(round(100 * m["citizen_sentiment"])))
        with prog2:
            st.caption("ê·œì œ ì••ë ¥")
            st.progress(int(round(100 * m["regulation_pressure"])))
        with prog3:
            st.caption("ê³µì •Â·ê·œì¹™ ë§Œì¡±")
            st.progress(int(round(100 * m["stakeholder_satisfaction"])))

        with st.expander("ğŸ“° ì‚¬íšŒì  ë°˜ì‘ í¼ì¹˜ê¸°"):
            st.write(f"ì§€ì§€ í—¤ë“œë¼ì¸: {nar.get('media_support_headline')}")
            st.write(f"ë¹„íŒ í—¤ë“œë¼ì¸: {nar.get('media_critic_headline')}")
            st.write(f"ì‹œë¯¼ ë°˜ì‘: {nar.get('citizen_quote')}")
            st.write(f"í”¼í•´ìÂ·ê°€ì¡± ë°˜ì‘: {nar.get('victim_family_quote')}")
            st.write(f"ê·œì œ ë‹¹êµ­ ë°œì–¸: {nar.get('regulator_quote')}")
            st.caption(nar.get("one_sentence_op_ed", ""))
        st.caption(f"ì„±ì°° ì§ˆë¬¸: {nar.get('followup_question', '')}")

        row = {
            "timestamp": dt.datetime.utcnow().isoformat(timespec="seconds"),
            "round": idx + 1,
            "scenario_id": scn.sid,
            "title": scn.title,
            "mode": mode,
            "choice": decision,
            "w_util": round(weights["emotion"], 3),
            "w_deon": round(weights["social"], 3),
            "w_cont": round(weights["moral"], 3),
            "w_virt": round(weights["identity"], 3),
            **{k: v for k, v in m.items()},
        }
        st.session_state.log.append(row)
        st.session_state.score_hist.append(m["ai_trust_score"])
        st.session_state.prev_trust = clamp(
            0.6 * st.session_state.prev_trust + 0.4 * m["social_trust"], 0, 1
        )

        if st.button("ë‹¤ìŒ ë¼ìš´ë“œ â–¶"):
            st.session_state.round_idx += 1
            st.session_state.last_out = None
            st.rerun()

# ==================== Footer / Downloads ====================
st.markdown("---")
st.subheader("ğŸ“¥ ë¡œê·¸ ë‹¤ìš´ë¡œë“œ")
if st.session_state.log:
    output = io.StringIO()
    writer = csv.DictWriter(output, fieldnames=list(st.session_state.log[0].keys()))
    writer.writeheader()
    writer.writerows(st.session_state.log)

    # âœ… íŒŒì¼ ì´ë¦„ ë’¤ì— íƒ€ì„ìŠ¤íƒ¬í”„ ë¶™ì´ê¸°
    timestamp = dt.datetime.now().strftime("%Y%m%d_%H%M%S")
    st.download_button(
        "CSV ë‚´ë ¤ë°›ê¸°",
        data=output.getvalue().encode("utf-8"),
        file_name=f"ethical_crossroads_log_{timestamp}.csv",
        mime="text/csv",
    )

st.caption("â€» ë³¸ ì•±ì€ êµìœ¡Â·ì—°êµ¬ìš© ì‚¬ê³ ì‹¤í—˜ì…ë‹ˆë‹¤. ì‹¤ì œ ìœ„í•´ í–‰ìœ„ë‚˜ ì°¨ë³„ì„ ê¶Œì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
